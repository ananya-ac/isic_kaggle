{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76447b41-843a-4f05-bd19-2971d5393fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split,  WeightedRandomSampler\n",
    "import os\n",
    "import torch\n",
    "import albumentations as A\n",
    "from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics.classification import F1Score, Precision, Recall\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from torchmetrics import Metric\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be2f402-4865-4fbc-8fb3-c61be059c4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7844b8b3-414b-4959-aa07-6fd99bbeca78",
   "metadata": {},
   "source": [
    "## Dataset/DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a19a666-d9b9-4c88-b51c-741a249df139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISIC_Dataset(Dataset):\n",
    "    def __init__(self, data_path, dataframe, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_name = os.path.join(self.data_path, self.dataframe['isic_id'].iloc[idx] + '.jpg')\n",
    "        \n",
    "        image = (np.array(Image.open(img_name).convert('RGB')) / 255)\n",
    "        target = self.dataframe['target'].iloc[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image'].transpose(2, 0, 1)\n",
    "            \n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "\n",
    "# class ISICDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, data_path, dataframe, batch_size=32, transform=None):\n",
    "#         super().__init__()\n",
    "#         self.data_path = data_path\n",
    "#         self.dataframe = dataframe\n",
    "#         self.batch_size = batch_size\n",
    "#         self.transform = transform\n",
    "#         self.train_df, self.val_df = self.get_train_test_dfs(test_size = 0.3)\n",
    "#         self.val_dataset = ISIC_Dataset(self.data_path, self.val_df, transform=self.transform)\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "        \n",
    "#         self.train_dataset = ISIC_Dataset(self.data_path, self.train_df, transform=self.transform)\n",
    "#         self.val_dataset = ISIC_Dataset(self.data_path, self.val_df, transform=self.transform)\n",
    "\n",
    "#     def get_train_test_dfs(self, test_size):\n",
    "#         X_df = self.dataframe.drop('target', axis = 'columns')\n",
    "#         y_df = self.dataframe['target']\n",
    "#         #y = self.dataframe['target'].values\n",
    "#         # Perform a stratified train-test split\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=test_size, stratify=y_df)\n",
    "        \n",
    "#         # Combine X and y into single DataFrames\n",
    "#         train_df = X_train.copy()\n",
    "#         train_df['target'] = y_train.values\n",
    "        \n",
    "#         val_df = X_test.copy()\n",
    "#         val_df['target'] = y_test.values\n",
    "\n",
    "#         return train_df, val_df\n",
    "        \n",
    "        \n",
    "\n",
    "#     def get_sampler(self, df):\n",
    "#         sample_weights = np.zeros(shape = (len(df)))\n",
    "#         vc = df['target'].value_counts()\n",
    "#         sample_weights[df['target'] == 1] = (1 / vc[1])\n",
    "#         sample_weights[df['target'] == 0] = (1 / vc[0])\n",
    "#         sampler = WeightedRandomSampler(weights = sample_weights, num_samples=len(sample_weights), replacement=False)\n",
    "#         return sampler\n",
    "        \n",
    "        \n",
    "#     def train_dataloader(self):\n",
    "#         sampler = self.get_sampler(self.train_df)\n",
    "#         return DataLoader(self.train_dataset, batch_size=self.batch_size,sampler = sampler, pin_memory = True, num_workers = 16)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         sampler = self.get_sampler(self.val_df)\n",
    "#         return DataLoader(self.val_dataset, batch_size=self.batch_size,sampler = sampler, pin_memory = True, num_workers = 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5abc2-75be-47e4-a4c1-1339634cdf55",
   "metadata": {},
   "source": [
    "## Model/LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b45a382-dcfa-44b4-be7a-8f0ae892c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class SaveBestModel(pl.Callback):\n",
    "    def __init__(self, filepath = './best_model/', monitor='val_loss', save_best_only=True):\n",
    "        \n",
    "        \n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best = float('inf')\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        current = trainer.callback_metrics[self.monitor].item()\n",
    "        \n",
    "        if self.save_best_only:\n",
    "            if current < self.best:\n",
    "                self.best = current\n",
    "                torch.save(pl_module.state_dict(), self.filepath)\n",
    "                print(f\"Validation {self.monitor} improved to {current:.4f}, saving model to {self.filepath}\")\n",
    "\n",
    "\n",
    "class pAUC(Metric):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.total_preds = []\n",
    "        self.labels = []\n",
    "\n",
    "    \n",
    "    def update(self, preds, target):\n",
    "        # Convert predictions to class labels\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        self.total_preds.extend(preds)\n",
    "        self.labels.extend(target)\n",
    "        \n",
    "    def compute(self, min_tpr = 0.8):\n",
    "        preds = torch.tensor(self.total_preds)\n",
    "        lbls = torch.tensor(self.labels)\n",
    "        \n",
    "        v_gt = abs(np.array(lbls.cpu())-1)\n",
    "        v_pred = -1.0*np.array(preds.cpu())\n",
    "    \n",
    "        max_fpr = abs(1-min_tpr)\n",
    "    \n",
    "        # using sklearn.metric functions: (1) roc_curve and (2) auc\n",
    "        fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n",
    "        if max_fpr is None or max_fpr == 1:\n",
    "            return auc(fpr, tpr)\n",
    "        if max_fpr <= 0 or max_fpr > 1:\n",
    "            raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n",
    "            \n",
    "        # Add a single point at max_fpr by linear interpolation\n",
    "        stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "        x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "        y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "        tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "        fpr = np.append(fpr[:stop], max_fpr)\n",
    "        partial_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #     # Equivalent code that uses sklearn's roc_auc_score\n",
    "        #     v_gt = abs(np.asarray(solution.values)-1)\n",
    "        #     v_pred = np.array([1.0 - x for x in submission.values])\n",
    "        #     max_fpr = abs(1-min_tpr)\n",
    "        #     partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "        #     # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "        #     # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "        #     partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "        \n",
    "        return torch.tensor(partial_auc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EfficientNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, efficientnet, num_features):\n",
    "        super(EfficientNetBinaryClassifier, self).__init__()\n",
    "\n",
    "        self.efficientnet = efficientnet\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        self.classifier = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.efficientnet(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x.view(-1, self.num_features))\n",
    "        return x\n",
    "\n",
    "class EfficientNetBinaryClassifierLightning(pl.LightningModule):\n",
    "    def __init__(self, efficientnet, val_loader, num_features, optim, lr=1e-3):\n",
    "        super(EfficientNetBinaryClassifierLightning, self).__init__()\n",
    "        self.model = EfficientNetBinaryClassifier(efficientnet, num_features)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.lr = lr\n",
    "        self.train_accuracy = Accuracy('binary')\n",
    "        self.val_accuracy = Accuracy('binary')\n",
    "        num_classes = 2\n",
    "        self.f1 = F1Score(num_classes=num_classes, average='weighted', task='binary')\n",
    "        self.precision = Precision(num_classes=num_classes, average='weighted', task='binary')\n",
    "        self.recall = Recall(num_classes=num_classes, average='weighted', task='binary')\n",
    "        self.p_auc = pAUC()\n",
    "        self.validation_dataloader = val_loader\n",
    "        self.optim_choice = optim\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(-1, 1).float()\n",
    "        #pdb.set_trace()\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = self.train_accuracy(preds, y.int())\n",
    "        labels = y.int()\n",
    "        f1 = self.f1(preds, labels)\n",
    "        self.precision(preds, labels)\n",
    "        self.recall(preds, labels)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch = True, on_step = True)\n",
    "        \n",
    "        # self.eval()  # Switch to evaluation mode\n",
    "        # # with torch.no_grad():  # Disable gradient computation\n",
    "        # #     for val_batch_idx, val_batch in enumerate(self.validation_dataloader):\n",
    "        # #         self.validation_step(val_batch, val_batch_idx)\n",
    "        # # self.train()  # Switch back to training mode\n",
    "        # x,y = self.validation_step()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(-1, 1).float()\n",
    "        \n",
    "        logits = self(x)\n",
    "        #pdb.set_trace()\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = self.val_accuracy(preds, y.int())\n",
    "        labels = y.int()\n",
    "        self.f1(preds, labels)\n",
    "        self.precision(preds, labels)\n",
    "        self.recall(preds, labels)\n",
    "        self.p_auc(preds, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch = True)\n",
    "        self.log('val_acc', acc, prog_bar=True, on_epoch = True)\n",
    "        self.log('val_f1', self.f1, prog_bar=True, on_epoch = True)\n",
    "        self.log('val_precision', self.precision, prog_bar=True, on_epoch = True)\n",
    "        self.log('val_recall', self.recall, prog_bar=True, on_epoch = True)\n",
    "        self.log('val_pAUC', self.p_auc, prog_bar=True, on_epoch=True)\n",
    "   \n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optim_choice == 'Adam':\n",
    "            return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        else:\n",
    "            return optim.SGD(model.parameters(), lr=self.lr, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d4dc093-45ef-4e08-8849-cfd442479ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dfs(dataframe,test_size):\n",
    "        X_df = dataframe.drop('target', axis = 'columns')\n",
    "        y_df = dataframe['target']\n",
    "        #y = self.dataframe['target'].values\n",
    "        # Perform a stratified train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=test_size, stratify=y_df)\n",
    "        \n",
    "        # Combine X and y into single DataFrames\n",
    "        train_df = X_train.copy()\n",
    "        train_df['target'] = y_train.values\n",
    "        \n",
    "        val_df = X_test.copy()\n",
    "        val_df['target'] = y_test.values\n",
    "\n",
    "        return train_df, val_df\n",
    "\n",
    "def get_sampler(df):\n",
    "        sample_weights = np.zeros(shape = (len(df)))\n",
    "        vc = df['target'].value_counts()\n",
    "        sample_weights[df['target'] == 1] = (1 / vc[1])\n",
    "        sample_weights[df['target'] == 0] = (1 / vc[0])\n",
    "        sampler = WeightedRandomSampler(weights = sample_weights, num_samples=len(sample_weights), replacement=False)\n",
    "        return sampler\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce50e09a-3f27-42a6-a8a2-9ae7f3231ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14004/1879230498.py:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_main = pd.read_csv('train-metadata.csv')\n",
      "/tmp/ipykernel_14004/1879230498.py:2: DtypeWarning: Columns (8,13,16,17,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_add = pd.read_csv('metadata.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "474497"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main = pd.read_csv('train-metadata.csv')\n",
    "df_add = pd.read_csv('metadata.csv')\n",
    "df_add = df_add[~df_add['benign_malignant'].isna()]\n",
    "df_add['benign_malignant'].value_counts()\n",
    "df_add = df_add[df_add['benign_malignant']!= 'indeterminate']\n",
    "df_add['target'] = df_add['benign_malignant'].apply(func = lambda x : 1 if 'malignant' in x else 0)\n",
    "common_cols = ['isic_id', 'target']\n",
    "data = pd.concat([df_add[common_cols], df_main[common_cols]], axis = 'rows')\n",
    "data_mini = pd.concat((data[data['target'] == 1], data[data['target'] == 0].iloc[:1000])).reset_index().sample(frac = 1)\n",
    "data_test = pd.read_csv('test-metadata.csv')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e03295d7-2cb7-4321-90de-3667262fc859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name           | Type                         | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | model          | EfficientNetBinaryClassifier | 63.8 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss            | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy               | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy               | 0      | train\n",
      "4 | f1             | BinaryF1Score                | 0      | train\n",
      "5 | precision      | BinaryPrecision              | 0      | train\n",
      "6 | recall         | BinaryRecall                 | 0      | train\n",
      "7 | p_auc          | pAUC                         | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "63.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "63.8 M    Total params\n",
      "255.158   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc017f1f8844721bf0a93f3efc7ffd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "data_path = './train-image/image/'\n",
    "transform = A.Compose([\n",
    "    A.Resize(100, 100),                    # Resize the image to 224x224\n",
    "    A.Rotate(limit=30, p=0.5)              # Rotate the image by ±30 degrees with 50% probability\n",
    "])\n",
    "batch_size = 32\n",
    "# Initialize the EfficientNet model (Assuming using EfficientNet-B0 for example)\n",
    "efficientnet = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\n",
    "num_features = efficientnet.classifier[1].in_features  # Assuming '_fc' is the final fully connected layer\n",
    "train_df, val_df = get_train_test_dfs(dataframe=data_mini, test_size = 0.3)\n",
    "        \n",
    "# Replace the final layer with an identity layer\n",
    "efficientnet.classifier = nn.Identity()\n",
    "train_dataset = ISIC_Dataset(data_path, train_df, transform=transform)\n",
    "val_dataset = ISIC_Dataset(data_path, val_df, transform=transform)\n",
    "train_sampler = get_sampler(train_df)\n",
    "val_sampler = get_sampler(val_df)\n",
    "# Initialize DataModule and Model\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler = train_sampler, pin_memory = True, num_workers = 16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler = val_sampler, pin_memory = True, num_workers = 16)\n",
    "#checkpoint_callback = SaveBestModel(monitor='val_loss', save_best_only=True)\n",
    "#data_module = ISICDataModule(data_path, dataframe, batch_size=32, transform=transform)\n",
    "model = EfficientNetBinaryClassifierLightning(efficientnet, val_loader, num_features, optim = 'Adam')\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "# Initialize Trainer\n",
    "trainer = pl.Trainer(logger = logger, max_epochs = 20, accelerator = 'auto')\n",
    "\n",
    "# Train and validate the model\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920ff6c-14b2-452b-abd5-51ba980d760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pAUC(Metric):\n",
    "    def __init__(self, compute_on_step=False, dist_sync_on_step=False, process_group=None, dist_sync_on_epoch=False):\n",
    "        super().__init__(compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step,\n",
    "                         process_group=process_group, dist_sync_on_epoch=dist_sync_on_epoch)\n",
    "\n",
    "        self.add_state(\"correct\", default_size=(1,), dist_reduce=\"sum\") \n",
    "        self.add_state(\"total\", default_size=(1,), dist_reduce=\"sum\") \n",
    "        self.total_preds = []\n",
    "        self.labels = [from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "]\n",
    "    \n",
    "    def update(self, preds, target):\n",
    "        # Convert predictions to class labels\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        self.correct += (preds == target).sum()\n",
    "        self.total += target.numel()\n",
    "\n",
    "    def compute(self):\n",
    "        v_gt = abs(np.asarray(solution.values)-1)\n",
    "    \n",
    "        # flip the submissions to their compliments\n",
    "        v_pred = -1.0*np.asarray(submission.values)\n",
    "    \n",
    "        max_fpr = abs(1-min_tpr)\n",
    "    \n",
    "        # using sklearn.metric functions: (1) roc_curve and (2) auc\n",
    "        fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n",
    "        if max_fpr is None or max_fpr == 1:\n",
    "            return auc(fpr, tpr)\n",
    "        if max_fpr <= 0 or max_fpr > 1:\n",
    "            raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n",
    "            \n",
    "        # Add a single point at max_fpr by linear interpolation\n",
    "        stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "        x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "        y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "        tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "        fpr = np.append(fpr[:stop], max_fpr)\n",
    "        partial_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #     # Equivalent code that uses sklearn's roc_auc_score\n",
    "        #     v_gt = abs(np.asarray(solution.values)-1)\n",
    "        #     v_pred = np.array([1.0 - x for x in submission.values])\n",
    "        #     max_fpr = abs(1-min_tpr)\n",
    "        #     partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "        #     # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "        #     # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "        #     partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "        \n",
    "        return(partial_auc)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e013c-3f17-41f8-a825-6b37e7b413d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1)\n",
    "    optim_choice = trial.suggest_categorical(\"optim_choice\", ['Adam', 'SGD'])\n",
    "    \n",
    "        \n",
    "    \n",
    "    data_path = './train-image/image/'\n",
    "    transform = A.Compose([\n",
    "        A.Resize(100, 100),                   \n",
    "        A.Rotate(limit=30, p=0.5)             \n",
    "    ])\n",
    "    batch_size = 32\n",
    "    # Initialize the EfficientNet model (Assuming using EfficientNet-B0 for example)\n",
    "    efficientnet = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\n",
    "    num_features = efficientnet.classifier[1].in_features  # Assuming '_fc' is the final fully connected layer\n",
    "    train_df, val_df = get_train_test_dfs(dataframe=data_mini, test_size = 0.3)\n",
    "            \n",
    "    # Replace the final layer with an identity layer\n",
    "    efficientnet.classifier = nn.Identity()\n",
    "    train_dataset = ISIC_Dataset(data_path, train_df, transform=transform)\n",
    "    val_dataset = ISIC_Dataset(data_path, val_df, transform=transform)\n",
    "    train_sampler = get_sampler(train_df)\n",
    "    val_sampler = get_sampler(val_df)\n",
    "    # Initialize DataModule and Model\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler = train_sampler, pin_memory = True, num_workers = 16)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler = val_sampler, pin_memory = True, num_workers = 16)\n",
    "    \n",
    "    #data_module = ISICDataModule(data_path, dataframe, batch_size=32, transform=transform)\n",
    "    model = EfficientNetBinaryClassifierLightning(efficientnet, val_loader, num_features, optim_choice, lr)\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "    # Initialize Trainer\n",
    "    trainer = pl.Trainer(logger = logger, max_epochs = 1, accelerator = 'auto', callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_pAUC\")])\n",
    "    \n",
    "    # Train and validate the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_pAUC\"].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f539d362-ea6e-433a-b96e-489926e947b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    464780\n",
       "1      9717\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18863c1-e774-4884-9020-4945cd94d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Lightning example.\")\n",
    "    parser.add_argument(\n",
    "        \"--pruning\",\n",
    "        \"-p\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Activate the pruning feature. `MedianPruner` stops unpromising \"\n",
    "        \"trials at the early stages of training.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner() if args.pruning else optuna.pruners.NopPruner()\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
